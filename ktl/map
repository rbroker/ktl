#pragma once

#include "kernel"
#include "algorithm"
#include "limits"
#include "tuple"
#include "memory"
#include "utility"
#include "hash_impl.h"

// Assuming x86, we can assume that everything after Windows 8 supports SSE2, at least.
#include <emmintrin.h>

namespace ktl
{
	// Based on ideas discussed in:
	// https://www.youtube.com/watch?v=ncHmEUmJZf4

	namespace internal
	{
		constexpr uint8_t MAP_CONTROL_EMPTY = 0x80;
		constexpr uint8_t MAP_CONTROL_PARTIAL_HASH_MASK = 0x7F;
		constexpr uint8_t MAP_CONTROL_PARTIAL_HASH_LENGTH = 0x7;

		struct _map_control
		{
			_map_control(uint8_t value) : control_byte_(value)
			{
			}

			bool is_hash(uint8_t truncatedHash) const
			{
				return control_byte_ == truncatedHash;
			}

			bool is_empty() const
			{
				return control_byte_ == MAP_CONTROL_EMPTY;
			}

			void erase()
			{
				control_byte_ = MAP_CONTROL_EMPTY;
			}

			uint8_t control_byte_ = MAP_CONTROL_EMPTY;
		};

		// Resizable array which doesn't own the elements it contains
		// Intended for use with the map, so that we can destroy elements in the pseudo vector
		// without needing to memmove etc.
		template<typename T>
		struct flat_map_data_array_iterator
		{
			using value_type = typename T;
			using reference = value_type&;
			using pointer = value_type*;

			flat_map_data_array_iterator() :
				begin_(nullptr),
				end_(nullptr),
				index_(numeric_limits<size_t>::max())
			{
			}

			flat_map_data_array_iterator(const pointer begin, const pointer end) :
				begin_(begin),
				end_(end),
				index_(0)
			{
			}

			flat_map_data_array_iterator(const pointer begin, const pointer end, size_t index) :
				begin_(begin),
				end_(end),
				index_(index)
			{
			}

			flat_map_data_array_iterator(const flat_map_data_array_iterator& other) :
				begin_(other.begin_),
				end_(other.end_),
				index_(other.index_)
			{
				KTL_TRACE_COPY_CONSTRUCTOR;
			}

			flat_map_data_array_iterator& operator=(const flat_map_data_array_iterator& other)
			{
				KTL_TRACE_COPY_ASSIGNMENT;

				begin_ = other.begin_;
				end_ = other.end_;
				index_ = other.index_;

				return *this;
			}

			reference operator*() const
			{
				return begin_[index_];
			}

			pointer operator->() const
			{
				return addressof(begin_[index_]);
			}

			flat_map_data_array_iterator& operator++()
			{
				if ((begin_ + ++index_) >= end_)
				{
					begin_ = nullptr;
					end_ = nullptr;
					index_ = numeric_limits<size_t>::max();
				}

				return *this;
			}

			flat_map_data_array_iterator operator++(int)
			{
				flat_map_data_array_iterator tmp = *this;
				++(*this);
				return tmp;
			}

			flat_map_data_array_iterator& operator--()
			{
				if (index_-- == 0)
				{
					begin_ = nullptr;
					end_ = nullptr;
					index_ = numeric_limits<size_t>::max();
				}

				return *this;
			}

			flat_map_data_array_iterator operator--(int)
			{
				flat_map_data_array_iterator tmp = *this;
				--(*this);
				return tmp;
			}

			[[nodiscard]] bool operator==(const flat_map_data_array_iterator& other) const
			{
				return begin_ == other.begin_ && end_ == other.end_ && index_ == other.index_;
			}

			[[nodiscard]] bool operator!=(const flat_map_data_array_iterator& other) const
			{
				return !(*this == other);
			}

			template<class T, class allocator_type>
			friend struct flat_map_data_array;

		private:
			pointer begin_;
			pointer end_;
			size_t index_ = 0;
		};

		template<class T, class allocator_type = paged_pool_allocator>
		struct flat_map_data_array
		{
			static_assert(is_base_of_v<generic_allocator, allocator_type>, "ktl::pseudo_vector requires allocator capable of arbitrary size allocations");

			using value_type = T;
			using iterator = flat_map_data_array_iterator<T>;

			flat_map_data_array() :
				a_{ allocator_type::instance() }
			{
			}

			flat_map_data_array(flat_map_data_array&& other) :
				capacity_(move(other.capacity_)),
				buffer_(move(other.buffer_)),
				a_(other.a_)
			{
				other.capacity_ = 0;
				other.buffer_ = nullptr;
			}

			~flat_map_data_array()
			{
				release_memory();
			}

			flat_map_data_array& operator=(flat_map_data_array&& other)
			{
				release_memory();

				buffer_ = move(other.buffer_);
				capacity_ = other.capacity_;

				other.capacity_ = 0;
				other.buffer_ = nullptr;
				return *this;
			}

			flat_map_data_array(const flat_map_data_array& other) = delete;
			flat_map_data_array& operator=(const flat_map_data_array& other) = delete;

			inline [[nodiscard]] size_t capacity() const
			{
				return capacity_;
			}

			[[nodiscard]] bool reserve(size_t newSize)
			{
				if (capacity() >= newSize)
					return true;

				// Obtain freshly-sized backing memory
				size_t control_bytes = (sizeof(_map_control) * newSize);
				size_t map_bytes = (sizeof(T) * newSize);
				T* tmp = reinterpret_cast<T*>(a_.allocate(control_bytes + map_bytes));
				if (!tmp)
					return false;

				static_assert(sizeof(_map_control) == sizeof(uint8_t));
				memset(tmp, MAP_CONTROL_EMPTY, control_bytes);

				// Free existing backing memory.
				release_memory();

				capacity_ = newSize;
				buffer_ = tmp;

				return true;
			}

			[[nodiscard]] _map_control* control()
			{
				return reinterpret_cast<_map_control*>(buffer_);
			}

			[[nodiscard]] T* map()
			{
				return reinterpret_cast<T*>(reinterpret_cast<_map_control*>(buffer_) + capacity());
			}

		private:
			void release_memory()
			{
				if (!buffer_)
					return;

				a_.deallocate(buffer_);
				buffer_ = nullptr;
			}

		private:
			size_t capacity_ = 0;
			T* buffer_ = nullptr;
			allocator_type& a_;
		};
	}

	template<class key_type, class value_type, class comparer, class allocator_type>
	struct flat_map;

	template<class key_type, class value_type, class comparer, class allocator_type>
	struct flat_map_iterator
	{
		using reference = tuple<key_type, value_type>&;
		using pointer = tuple<key_type, value_type>*;

		flat_map_iterator()
			: map_{ nullptr }
			, index_ { numeric_limits<size_t>::max() }
		{
		}

		flat_map_iterator(observer_ptr<flat_map<key_type, value_type, comparer, allocator_type>> map, size_t index)
			: map_{ map }
			, index_{ index }
		{
		}

		flat_map_iterator(const flat_map_iterator& other) :
			map_(other.map_),
			index_(other.index_)
		{
			KTL_TRACE_COPY_CONSTRUCTOR;
		}

		flat_map_iterator& operator=(const flat_map_iterator& other)
		{
			KTL_TRACE_COPY_ASSIGNMENT;

			map_ = other.map_;
			index_ = other.index_;

			return *this;
		}

		reference operator*() const
		{
			return map_->backing_.map()[index_];
		}

		pointer operator->() const
		{
			return addressof(map_->backing_.map_()[index_]);
		}

		flat_map_iterator& operator++()
		{
			next();

			return *this;
		}

		flat_map_iterator operator++(int)
		{
			flat_map_iterator tmp = *this;
			++(*this);
			return tmp;
		}

		bool operator==(const flat_map_iterator& other)
		{
			return map_ == other.map_ && index_ == other.index_;
		}

		bool operator!=(const flat_map_iterator& other)
		{
			return !(*this == other);
		}

	private:
		void next()
		{
			auto control = map_->backing_.control();
			size_t capacity = map_->capacity();

			// Try to find the next element by searching for an in-use slot
			// in the control bytes.
			do
			{
				++index_;

				if (index_ < capacity)
				{
					// If we found an element, great. Stop probing.
					if (!control[index_].is_empty())
						return;
				}
			} while (index_ < capacity);

			// We didn't find an element, so set ourselves as end()
			index_ = numeric_limits<size_t>::max();
			map_ = nullptr;
		}

	private:
		observer_ptr<flat_map<key_type, value_type, comparer, allocator_type>> map_;
		size_t index_;
	};

	template<class key_type, class value_type, class comparer = equal_to<key_type>, class allocator_type = paged_pool_allocator>
	struct flat_map
	{
		using iterator = flat_map_iterator<key_type, value_type, comparer, allocator_type>;
		using element_type = tuple<key_type, value_type>;
		friend struct iterator;

		~flat_map()
		{
			clear();
		}

		iterator insert(key_type&& key, value_type&& value)
		{
			// https://docs.microsoft.com/en-us/windows-hardware/drivers/kernel/floating-point-support-for-64-bit-drivers
			// Save SSE register states, in case we're x86.
			sse_state saved_state;

			if (!try_grow())
				return iterator{};

			tuple t{ move(key), move(value) };

			size_t index = insert_impl(backing_.control(), backing_.map(), move(t), backing_.capacity());

			if (index == numeric_limits<size_t>::max())
				return iterator{};
			else
				return iterator(this, index);
		}

		iterator insert(key_type const& key, value_type const& value)
		{
			// https://docs.microsoft.com/en-us/windows-hardware/drivers/kernel/floating-point-support-for-64-bit-drivers
			// Save SSE register states, in case we're x86.
			sse_state saved_state;

			if (!try_grow())
				return iterator{};

			tuple t{ key, value };

			size_t index = insert_impl(backing_.control(), backing_.map(), move(t), backing_.capacity());

			if (index == numeric_limits<size_t>::max())
				return iterator{};
			else
				return iterator(this, index);
		}

		iterator find(const key_type& key)
		{
			// https://docs.microsoft.com/en-us/windows-hardware/drivers/kernel/floating-point-support-for-64-bit-drivers
			// Save SSE register states, in case we're x86.
			sse_state saved_state;

			size_t index = find_impl(key);

			if (index == numeric_limits<size_t>::max())
				return iterator{};
			else
				return iterator(this, index);
		}

		size_t capacity() const
		{
			return backing_.capacity();
		}

		size_t size() const
		{
			return size_;
		}

		void clear()
		{
			size_t cap = capacity();
			auto control = backing_.control();
			auto map = backing_.map();

			size_t i = 0;
			constexpr size_t SIMD_CHUNK_SIZE = sizeof(__m128i) / sizeof(uint8_t);
			auto empty_mask = _mm_set1_epi8(internal::MAP_CONTROL_EMPTY);

			while (i < cap)
			{
				if ((cap - i) >= SIMD_CHUNK_SIZE)
				{
					auto chunk = _mm_loadu_si128(reinterpret_cast<__m128i*>(addressof(control[i])));
					auto empty_match = _mm_cmpeq_epi8(chunk, empty_mask);
					int not_empty_mask = ~(_mm_movemask_epi8(empty_match)) & 0xFFFF;

					// Scan though all non-empty control bytes and destroy the corresponding elements.
					unsigned long indexOffset;
					while (BitScanForward(&indexOffset, not_empty_mask))
					{
						auto tmpIndex = i + indexOffset;
						remove_element(control, map, tmpIndex);
						not_empty_mask &= ~(1 << indexOffset);
					}

					i += SIMD_CHUNK_SIZE;
				}
				else
				{
					const auto& c = control[i];
					if (!c.is_empty())
					{
						remove_element(control, map, i);
					}

					++i;
				}
			}
		}

		/// <summary>
		/// Remove the element with the given key from the map.
		/// </summary>
		iterator erase(const key_type& key)
		{
			size_t index = find_impl(key);

			if (index == numeric_limits<size_t>::max())
				return iterator{};

			remove_element(backing_.control(), backing_.map(), index);
			--size_;

			auto it = iterator{ this, index };
			return ++it;
		}

		iterator begin() const
		{
			if (size() > 0)
			{
				return iterator{ this, 0 };
			}
			else
			{
				return end();
			}
		}

		iterator end() const
		{
			return iterator{};
		}

		/// <summary>
		/// Shrinks the map such that it occupies the smallest possible (power of 2) size which doesn't
		/// exceed the max load factor.
		/// Will incur a rehashing cost and temporarily increased memory usage while shrinking.
		/// </summary>
		[[nodiscard]] bool shrink_to_fit()
		{
			size_t newCapacity = capacity();
			const double sz = static_cast<double>(size());

			floating_point_state state;

			while (newCapacity > 0)
			{
				size_t c = newCapacity >> 1;

				if (sz / c <= max_load_factor())
				{
					newCapacity = c;
				}
				else
				{
					break;
				}
			}

			// Don't do an expensive rehash unless we need to.
			if (newCapacity >= capacity() || newCapacity == 0)
				return true;

			return rehash(newCapacity);
		}

		[[nodiscard]] bool reserve(size_t newCapacity)
		{
			if (newCapacity <= capacity())
				return true;

			// Only accept 2^ table sizes.
			if ((newCapacity & (newCapacity - 1)) != 0)
			{
				return false;
			}

			return rehash(newCapacity);
		}

	private:
		// Fast modulus, requires power of two divisor.
		inline size_t fast_modulo(size_t val, size_t divisor)
		{
			return val & (divisor - 1);
		}

		void remove_element(internal::_map_control* control, tuple<key_type, value_type>* map, size_t index)
		{
			control[index].erase();

			// If this type needs non-trivial destruction, then do something with
			// it, otherwise we can just leave it in the map_ with the control byte
			// indicating empty.
			if constexpr (!is_trivially_destructible_v<tuple<key_type, value_type>>)
			{
				map[index].~tuple();
			}

			--size_;
		}

		bool rehash(size_t newCapacity)
		{
			internal::flat_map_data_array<tuple<key_type, value_type>, allocator_type> newBacking;
			if (!newBacking.reserve(newCapacity))
				return false;

			size_ = 0; // We'll be re-inserting all elements which will increment size, so just drop ourselves back down to "empty" before re-insertion.
			size_t cap = capacity();
			auto control = backing_.control();
			auto map = backing_.map();
			auto newControl = newBacking.control();
			auto newMap = newBacking.map();

			for (size_t index = 0; index < cap; ++index)
			{
				auto& c = control[index];
				if (!c.is_empty())
				{
					insert_impl(newControl, newMap, move(map[index]), newCapacity);
				}
			}

			backing_ = move(newBacking);

			return true;
		}

		__forceinline size_t insert_impl(internal::_map_control* control, tuple<key_type, value_type>* map, tuple<key_type, value_type>&& t, size_t map_capacity)
		{
			constexpr size_t SIMD_CHUNK_SIZE = sizeof(__m128i) / sizeof(uint8_t);
			const auto& [key, value] = t;
			const auto h = hash<key_type>{}(key);
			const uint8_t truncated_hash = h & internal::MAP_CONTROL_PARTIAL_HASH_MASK;
			size_t index = fast_modulo(h >> internal::MAP_CONTROL_PARTIAL_HASH_LENGTH, map_capacity);
			const size_t endIndex = index;

			const __m128i probe_hash_mask = _mm_set1_epi8(truncated_hash);

			// Probe each element in the map at least once.
			do
			{
				if ((map_capacity - index) >= SIMD_CHUNK_SIZE)
				{
					__m128i control_chunk = _mm_loadu_si128(reinterpret_cast<__m128i*>(addressof(control[index])));

					// Check for duplicate keys
					__m128i probe_hash_match = _mm_cmpeq_epi8(control_chunk, probe_hash_mask);
					int matchMask = _mm_movemask_epi8(probe_hash_match);
					unsigned long indexOffset;

					// Scan all hash matches to see if we have a match.
					while (BitScanForward(&indexOffset, matchMask))
					{
						auto tmpIndex = index + indexOffset;
						const auto& [element_key, element_value] = map[tmpIndex];

						if (comparer()(element_key, key)) [[likely]]
						{
							if constexpr (!is_trivially_destructible_v<tuple<key_type, value_type>>)
							{
								map[tmpIndex].~tuple();
							}

							(void) construct_at<element_type>(addressof(map[tmpIndex]), move(t));
							return tmpIndex;
						}

						// Disable bit we just checked in case we loop around again.
						matchMask &= ~(1 << indexOffset);
					}

					// We only need to find a single empty element for an insertion.
					__m128i probe_empty_mask = _mm_set1_epi8(internal::MAP_CONTROL_EMPTY);
					__m128i probe_empty_match = _mm_cmpeq_epi8(control_chunk, probe_empty_mask);
					matchMask = _mm_movemask_epi8(probe_empty_match);

					if (BitScanForward(&indexOffset, matchMask))
					{
						auto tmpIndex = index + indexOffset;

						++size_;
						(void)construct_at<element_type>(addressof(map[tmpIndex]), move(t));
						control[tmpIndex] = truncated_hash;
						return tmpIndex;
					}

					// If we didn't get a match, check the next chunk.
					index = fast_modulo(index + SIMD_CHUNK_SIZE, map_capacity);
				}
				else
				{
					// Potential duplicate key.
					if (control[index].is_hash(truncated_hash))
					{
						const auto& [element_key, element_value] = map[index];

						if (comparer()(element_key, key)) [[likely]]
						{
							if constexpr (!is_trivially_destructible_v<element_type>)
							{
								map[index].~tuple();
							}

							(void)construct_at<element_type>(addressof(map[index]), move(t));
							return index;
						}
					}

					// We found an empty slot in the control map. Insert here.
					if (control[index].is_empty())
					{
						++size_;
						(void)construct_at<element_type>(addressof(map[index]), move(t));
						control[index] = truncated_hash;
						return index;
					}

					// wrap back to the start of the map if we reach the end.
					index = fast_modulo(index + 1, map_capacity);
				}
			} while (index != endIndex);

			return numeric_limits<size_t>::max();
		}

		__forceinline bool sse2_any_control_bytes_empty(__m128i* chunk)
		{
			// If there's any empty elements in this chunk, we can abandon our search.
			const __m128i probe_empty_mask = _mm_set1_epi8(internal::MAP_CONTROL_EMPTY);
			const __m128i probe_empty_match = _mm_cmpeq_epi8(*chunk, probe_empty_mask);
			return _mm_movemask_epi8(probe_empty_match) != 0;
		}

		__forceinline size_t find_impl(const key_type& key)
		{
			const auto h = hash<key_type>{}(key);
			const uint8_t truncated_hash = h & internal::MAP_CONTROL_PARTIAL_HASH_MASK;
			const size_t map_capacity = capacity();
			size_t index = fast_modulo(h >> internal::MAP_CONTROL_PARTIAL_HASH_LENGTH, map_capacity);
			const size_t endIndex = index;
			auto control = backing_.control();

			constexpr size_t SIMD_CHUNK_SIZE = sizeof(__m128i) / sizeof(uint8_t);
			const __m128i probe_hash_mask = _mm_set1_epi8(truncated_hash);

			// Probe each element at least once.
			do
			{
				if ((map_capacity - index) >= SIMD_CHUNK_SIZE)
				{
					__m128i control_chunk = _mm_loadu_si128(reinterpret_cast<__m128i*>(&(control[index])));

					// Check for hash matches in this block.
					const __m128i probe_hash_match = _mm_cmpeq_epi8(control_chunk, probe_hash_mask);
					int matchMask = _mm_movemask_epi8(probe_hash_match);
					unsigned long indexOffset;
					// Scan all hash matches to see if we have a match.
					while (BitScanForward(&indexOffset, matchMask))
					{
						auto tmpIndex = index + indexOffset;
						const auto& [element_key, element_value] = backing_.map()[tmpIndex];

						if (comparer()(element_key, key)) [[likely]]
							return tmpIndex;

						// Disable the bit we just checked before continuing.
						matchMask &= ~(1 << indexOffset);
					}

					// If there's any empty elements in this chunk, and we haven't found a matching
					// element yet, we can abandon our search.
					if (sse2_any_control_bytes_empty(&control_chunk))
						break;

					// If we didn't get a match, check the next chunk.
					index = fast_modulo(index + SIMD_CHUNK_SIZE, map_capacity);
				}
				else
				{
					if (control[index].is_hash(truncated_hash))
					{
						const auto& [element_key, element_value] = backing_.map()[index];

						if (comparer()(element_key, key)) [[likely]]
							return index;
					}

					// If our probe hit an empty map element, then the value
					// we're looking for can't be in this map.
					if (control[index].is_empty())
						break;

					// wrap back to the start of the map if we reach the end.
					index = fast_modulo(index + 1, map_capacity);
				}
			} while (index != endIndex);

			return numeric_limits<size_t>::max();
		}

		/// Returns the load factor of the map. The caller *must* perform this call and any
		/// subsequent calculations in a scope containing a ktl::floating_point_state object
		[[nodiscard]] double load_factor() const
		{
			return static_cast<double>(size()) / capacity();
		}

		/// Returns the maximum load factor of the map. The caller *must* perform this call and any
		/// subsequent calculations in a scope containing a ktl::floating_point_state object
		[[nodiscard]] double max_load_factor() const
		{
			return 0.8;
		}

		[[nodiscard]] bool try_grow()
		{
			auto c = capacity();

			if (c == 0)
			{
				return reserve(2);
			}
			else
			{
				{
					floating_point_state state;

					if (load_factor() < max_load_factor())
						return true;
				}

				return reserve(c * 2);
			}
		}

	private:
		size_t size_ = 0;
		internal::flat_map_data_array<tuple<key_type, value_type>, allocator_type> backing_;
	};

	template<class key_type, class value_type, class comparer, class allocator_type>
	flat_map_iterator<key_type, value_type, comparer, allocator_type> begin(flat_map<key_type, value_type, comparer, allocator_type>& m)
	{
		return m.begin();
	}

	template<class key_type, class value_type, class comparer, class allocator_type>
	flat_map_iterator<key_type, value_type, comparer, allocator_type> end(flat_map<key_type, value_type, comparer, allocator_type>& m)
	{
		return m.end();
	}
}