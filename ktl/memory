#pragma once

#include <ktl_core.h>

#include <new>
#include <type_traits>

namespace ktl
{
	template<class T, class... Args>
	[[nodiscard]] constexpr T* construct_at(void* p, Args&&... args)
	{
		return new (p) T(forward<Args>(args)...);
	}

	template<class T, class allocator_type, class... Args>
	[[nodiscard]] constexpr T* construct(allocator_type& a, Args&&... args)
	{
		auto p = a.allocate(sizeof(T));

		if (p == nullptr)
			return nullptr;

		return construct_at<T>(p, forward<Args>(args)...);
	}

	template<class T, class allocator_type, enable_if_t<!is_array_v<T>, int> = 0>
	constexpr void destroy(allocator_type& a, T* p)
	{
		// Destroy the object.
		p->~T();

		// Free the memory.
		a.deallocate(p);
	}

	template <class T>
	[[nodiscard]] constexpr T* addressof(T& value) noexcept {
		return __builtin_addressof(value);
	}

	struct allocator
	{
		virtual ~allocator() {};

		virtual void* allocate(size_t n) = 0;
		virtual void deallocate(void* p) = 0;

		void validate(const char* msg)
		{
#if KTL_TRACK_ALLOCATIONS
			if (allocationCount_ != deallocationCount_)
			{
				KTL_LOG_ERROR("%s: alloc count (%llu) doesn't match dealloc count (%llu)\n", msg, allocationCount_, deallocationCount_);
			}
			else
			{
				KTL_LOG_TRACE("%s: allocation counts OK\n", msg);
			}
#else
			UNREFERENCED_PARAMETER(msg);
#endif
		}

#if KTL_TRACK_ALLOCATIONS
	protected:
		volatile LONG64 allocationCount_ = 0;
		volatile LONG64 deallocationCount_ = 0;
#endif
	};

	struct generic_allocator : public allocator {};
	struct fixed_size_allocator : public allocator {};

	struct paged_pool_allocator : public generic_allocator
	{
		paged_pool_allocator() = default;

		paged_pool_allocator(const paged_pool_allocator&) = delete;
		paged_pool_allocator(paged_pool_allocator&&) = delete;

		paged_pool_allocator& operator=(const paged_pool_allocator&) = delete;
		paged_pool_allocator& operator=(paged_pool_allocator&&) = delete;

		~paged_pool_allocator()
		{
#if KTL_TRACK_ALLOCATIONS
			validate(__FUNCTION__);
#endif
		}

		[[nodiscard]] void* allocate(size_t n) override
		{
			auto p = pool_alloc(n, pool_type::Paged);
#if KTL_TRACK_ALLOCATIONS
			if (p)
			{
				InterlockedIncrement64(&allocationCount_);
			}
#endif
			return p;
		}

		void deallocate(void* p) override
		{
#if KTL_TRACK_ALLOCATIONS
			InterlockedIncrement64(&deallocationCount_);
#endif
			pool_free(p);
		}

		static paged_pool_allocator& instance()
		{
			static paged_pool_allocator a = {};
			return a;
		}
	};

	struct nonpaged_pool_allocator : public generic_allocator
	{
		nonpaged_pool_allocator() = default;

		nonpaged_pool_allocator(const nonpaged_pool_allocator&) = delete;
		nonpaged_pool_allocator(nonpaged_pool_allocator&&) = delete;

		nonpaged_pool_allocator& operator=(const nonpaged_pool_allocator&) = delete;
		nonpaged_pool_allocator& operator=(nonpaged_pool_allocator&&) = delete;

		~nonpaged_pool_allocator()
		{
#if KTL_TRACK_ALLOCATIONS
			validate(__FUNCTION__);
#endif
		}

		[[nodiscard]] void* allocate(size_t n) override
		{
			auto p = pool_alloc(n, pool_type::NonPaged);
#if KTL_TRACK_ALLOCATIONS
			if (p)
			{
				InterlockedIncrement64(&allocationCount_);
			}
#endif
			return p;
		}

		void deallocate(void* p) override
		{
#if KTL_TRACK_ALLOCATIONS
			InterlockedIncrement64(&deallocationCount_);
#endif
			pool_free(p);
		}

		static nonpaged_pool_allocator& instance()
		{
			static nonpaged_pool_allocator a = {};
			return a;
		}
	};

	template<size_t BLOCK_SIZE>
	struct paged_lookaside_allocator : public fixed_size_allocator
	{
		paged_lookaside_allocator()
		{
			ExInitializeLookasideListEx(
				&lookaside_,
				nullptr,
				nullptr,
				POOL_TYPE::PagedPool,
				EX_LOOKASIDE_LIST_EX_FLAGS_RAISE_ON_FAIL,
				BLOCK_SIZE,
				KTL_POOL_TAG,
				0
			);
		}

		paged_lookaside_allocator(const paged_lookaside_allocator&) = delete;
		paged_lookaside_allocator(paged_lookaside_allocator&&) = delete;

		paged_lookaside_allocator& operator=(const paged_lookaside_allocator&) = delete;
		paged_lookaside_allocator& operator=(paged_lookaside_allocator&&) = delete;

		~paged_lookaside_allocator()
		{
#if KTL_TRACK_ALLOCATIONS
			validate(__FUNCTION__);
#endif
			ExDeleteLookasideListEx(&lookaside_);
		}

		[[nodiscard]] void* allocate(size_t n) override
		{
			if (n > BLOCK_SIZE)
			{
				KTL_LOG_ERROR("Invalid paged_lookaside allocation: %llu > %llu", n, BLOCK_SIZE);
				return nullptr;
			}

			__try
			{
				auto p = ExAllocateFromLookasideListEx(&lookaside_);

#if KTL_TRACK_ALLOCATIONS
				if (p)
				{
					InterlockedIncrement64(&allocationCount_);
				}
#endif

				return p;
			}
			__except (EXCEPTION_EXECUTE_HANDLER)
			{
				return nullptr;
			}
		}

		void deallocate(void* p) override
		{
#if KTL_TRACK_ALLOCATIONS
			InterlockedIncrement64(&deallocationCount_);
#endif
			ExFreeToLookasideListEx(&lookaside_, p);
		}

		static paged_lookaside_allocator& instance()
		{
			static paged_lookaside_allocator<BLOCK_SIZE> a = {};
			return a;
		}

	private:
		alignas(16) LOOKASIDE_LIST_EX lookaside_;
	};

	template<size_t BLOCK_SIZE>
	struct nonpaged_lookaside_allocator : public fixed_size_allocator
	{
		nonpaged_lookaside_allocator()
		{
			ExInitializeLookasideListEx(
				&lookaside_,
				nullptr,
				nullptr,
				POOL_TYPE::NonpagedPoolNx,
				EX_LOOKASIDE_LIST_EX_FLAGS_RAISE_ON_FAIL,
				BLOCK_SIZE,
				KTL_POOL_TAG,
				0
			);
		}

		nonpaged_lookaside_allocator(const nonpaged_lookaside_allocator&) = delete;
		nonpaged_lookaside_allocator(nonpaged_lookaside_allocator&&) = delete;

		nonpaged_lookaside_allocator& operator=(const nonpaged_lookaside_allocator&) = delete;
		nonpaged_lookaside_allocator& operator=(nonpaged_lookaside_allocator&&) = delete;

		~nonpaged_lookaside_allocator()
		{
#if KTL_TRACK_ALLOCATIONS
			validate(__FUNCTION__);
#endif
			ExDeleteLookasideListEx(&lookaside_);
		}

		[[nodiscard]] void* allocate(size_t n) override
		{
			if (n > BLOCK_SIZE)
			{
				KTL_LOG_ERROR("Invalid nonpaged_lookaside allocation: %llu > %llu", n, BLOCK_SIZE);
				return nullptr;
			}

			__try
			{
				auto p = ExAllocateFromLookasideListEx(&lookaside_);
#if KTL_TRACK_ALLOCATIONS
				if (p)
				{
					InterlockedIncrement64(&allocationCount_);
				}
#endif
				return p;
			}
			__except (EXCEPTION_EXECUTE_HANDLER)
			{
				return nullptr;
			}
		}

		void deallocate(void* p) override
		{
#if KTL_TRACK_ALLOCATIONS
			InterlockedIncrement64(&deallocationCount_);
#endif
			ExFreeToLookasideListEx(&lookaside_, p);
		}

		static nonpaged_lookaside_allocator& instance()
		{
			static nonpaged_lookaside_allocator<BLOCK_SIZE> a = {};
			return a;
		}

	private:
		alignas(16) LOOKASIDE_LIST_EX lookaside_;
	};

	// Scalar unique_ptr
	template<class T>
	struct unique_ptr
	{
		using value_type = remove_reference_t<T>;
		using pointer = value_type*;
		using reference = value_type&;

		unique_ptr()
		{
		}

		unique_ptr(pointer p)
		{
			reset(p);
		}

		unique_ptr(unique_ptr&& other)
		{
			reset(other.release());
		}

		unique_ptr& operator=(unique_ptr&& other)
		{
			reset(other.release());
			return *this;
		}

		reference operator*() const
		{
			return *p_;
		}

		pointer operator->() const
		{
			return p_;
		}

		explicit operator bool() const
		{
			return p_ != nullptr;
		}

		unique_ptr(const unique_ptr&) = delete;
		unique_ptr& operator=(const unique_ptr&) = delete;

		~unique_ptr()
		{
			reset();
		}

		[[nodiscard]] pointer get() const
		{
			return p_;
		}

		void reset(pointer newPtr = nullptr)
		{
			pointer oldPtr = p_;
			p_ = newPtr;

			if (oldPtr != nullptr)
			{
				delete oldPtr;
				oldPtr = nullptr;
			}
		}

		pointer release()
		{
			pointer p = p_;
			p_ = nullptr;
			return p;
		}

	private:
		pointer p_ = nullptr;
	};

	// Array unique_ptr
	template<class T>
	struct unique_ptr<T[]>
	{
		using value_type = remove_reference_t<remove_extent_t<T>>;
		using pointer = value_type*;
		using reference = value_type&;
		using element_type = T;

		unique_ptr()
		{
		}

		unique_ptr(pointer p)
		{
			reset(p);
		}

		unique_ptr(unique_ptr&& other)
		{
			reset(other.release());
		}

		unique_ptr& operator=(unique_ptr&& other)
		{
			reset(other.release());
			return *this;
		}

		reference operator*() const
		{
			return *p_;
		}

		pointer operator->() const
		{
			return p_;
		}

		explicit operator bool() const
		{
			return p_ != nullptr;
		}

		unique_ptr(const unique_ptr&) = delete;
		unique_ptr& operator=(const unique_ptr&) = delete;

		~unique_ptr()
		{
			reset();
		}

		[[nodiscard]] pointer get() const
		{
			return p_;
		}

		void reset(element_type* newPtr = nullptr)
		{
			element_type* oldPtr = p_;
			p_ = newPtr;

			if (oldPtr != nullptr)
			{
				delete[] oldPtr;
				oldPtr = nullptr;
			}
		}

		pointer release()
		{
			pointer p = p_;
			p_ = nullptr;
			return p;
		}

		reference operator[](size_t index)
		{
			return p_[index];
		}

	private:
		pointer p_ = nullptr;
	};

	template<typename T>
	struct observer_ptr
	{
		using value_type = remove_extent_t<remove_reference_t<T>>;
		using pointer = value_type*;
		using reference = value_type&;

		observer_ptr() :
			p_(nullptr)
		{
		}

		observer_ptr(const pointer p) :
			p_(p)
		{
		}

		observer_ptr(const observer_ptr& other) :
			p_(other.p_)
		{
			KTL_TRACE_COPY_CONSTRUCTOR;
		}

		observer_ptr& operator=(const observer_ptr& other)
		{
			KTL_TRACE_COPY_ASSIGNMENT;

			p_ = other.p_;
			return *this;
		}

		observer_ptr& operator=(pointer p)
		{
			reset(p);
			return *this;
		}

		reference operator*() const
		{
			return *p_;
		}

		pointer operator->() const
		{
			return p_;
		}

		pointer* operator&() const
		{
			return &p_;
		}

		explicit operator bool() const
		{
			return p_ != nullptr;
		}

		pointer get() const
		{
			return p_;
		}

		void reset(pointer newPtr = nullptr)
		{
			p_ = newPtr;
		}

		pointer release()
		{
			pointer tmp = p_;
			p_ = nullptr;
			return tmp;
		}

		[[nodiscard]] bool has_value() const
		{
			return p_ != nullptr;
		}

		[[nodiscard]] value_type& value()
		{
			return *p_;
		}

		[[nodiscard]] bool operator==(const observer_ptr<T>& other) const
		{
			return p_ == other.p_;
		}

	private:
		pointer p_;
	};

	// ktl::make_unique - scalar
	template<class T, class... Args, enable_if_t<!is_array_v<T>, int> = 0>
	[[nodiscard]] unique_ptr<T> make_unique(pool_type pool, Args&&... args)
	{
		return unique_ptr<T>(new (pool) T(forward<Args>(args)...));
	}

	// ktl::make_unique - array
	template<class T, enable_if_t<is_array_v<T> && extent_v<T> == 0, int> = 0>
	[[nodiscard]] unique_ptr<T> make_unique(pool_type pool, size_t size)
	{
		return unique_ptr<T>(new (pool) remove_extent_t<T>[size]());
	}
}